{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a Dictionary with 2500 keys and setting their value to 1. The reason of putting the value of\n",
    "#1 instead of zero is because of the laplace smoothing of the numerator.\n",
    "#pickle module helps in seralization of data. It is easier to load data.\n",
    "\n",
    "dic1={}          #dic1 contains words appeared in non spam emails.\n",
    "dic2={}          #dic2 contains words appeared in spam emails.\n",
    "for i in range(1,2501):\n",
    "    dic1.update({i:1})\n",
    "    dic2.update({i:1})\n",
    "k=[dic1,dic2]\n",
    "with open(\"dic.pickle\",\"wb\") as f:\n",
    "    pickle.dump(k,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"dic.pickle\",\"rb\") as f:\n",
    "    k=pickle.load(f)                                        #k[0] contains words appeared in non spam emails.\n",
    "                                                            #k[1] contains words appeared in spam emails.\n",
    "v=2500\n",
    "df=pd.read_csv(\"train-features.txt\", sep=' ',\n",
    "                  names = [\"DocID\", \"DicID\", \"Occ\"])\n",
    "s=df[\"DocID\"]\n",
    "\n",
    "#reading the file and giving them respective headers\n",
    "#DocId- Document number,DicID-Dictionary token number (1-2500),Occ-No. of times occured in the respective document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Training the classifier\n",
    "c=1\n",
    "r=0                       #Counting the length of each words in the document\n",
    "a=[]                       #a is a list of all the lengths of document like a[0] is the no. of words in first document\n",
    "for i in range(len(s)):\n",
    "    if (s[i])==c:\n",
    "        r+=df[\"Occ\"][i]\n",
    "    else:\n",
    "        a.append(r)\n",
    "        c+=1                                     \n",
    "        r=r-r\n",
    "        r+=df[\"Occ\"][i] \n",
    "a.append(r)\n",
    "b=a[0:350]             #Dividing the lenghts into two lists. As 0-350 documents are not spam(0) and 350-700 are spam(1) \n",
    "a=a[350:700]\n",
    "nsp=sum(b)+v   #v is length of the dictionary ie 2500, it is added due to laplace smoothing\n",
    "sp=sum(a)+v\n",
    "sums=[nsp,sp]\n",
    "with open(\"dicsum.pickle\",\"wb\") as f:\n",
    "   pickle.dump(sums,f)\n",
    "\n",
    "sums=[]\n",
    "with open(\"dicsum.pickle\",\"rb\") as f:\n",
    "   sums=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(s)):              #Updating the non spam and spam dictionary by adding the occurance of the word.\n",
    "    if int(s[i])<=350:\n",
    "        k[0][(df[\"DicID\"][i])]+=df[\"Occ\"][i]\n",
    "    else:\n",
    "        k[1][(df[\"DicID\"][i])]+=df[\"Occ\"][i]\n",
    "            \n",
    "with open(\"classydicl.pickle\",\"wb\") as f:\n",
    "   pickle.dump(k,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"classydicl.pickle\",\"rb\") as f:\n",
    "    q=pickle.load(f)                    #Our numerator and denominator are both ready.Now we Divide.\n",
    "\n",
    "for keys in (q[0]):\n",
    "    q[0][keys]=np.divide(q[0][keys],sums[0])\n",
    "    q[1][keys]=np.divide(q[1][keys],sums[1])\n",
    "    \n",
    "\n",
    "with open(\"newclassydic.pickle\",\"wb\") as f:\n",
    "   pickle.dump(q,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"newclassydic.pickle\",\"rb\") as f:\n",
    "    k=pickle.load(f)\n",
    "#newclassydic is our trained classifier\n",
    "#k loads the new classifier k[0] contains non spam and k[1] contains spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Naive Bayes Algorithm is 98.46153846153847\n"
     ]
    }
   ],
   "source": [
    "##Testing The Naive Bayes Classifier\n",
    "\n",
    "df=pd.read_csv(\"test-features.txt\", sep=' ',\n",
    "                  names = [\"DocID\", \"DicID\", \"Occ\"])  #reading the file and giving them respective headers\n",
    "s=df[\"DocID\"]\n",
    "t=df[\"DicID\"]\n",
    "u=df[\"Occ\"]\n",
    "x=np.log(0.50)                  #0.50 is the probability of spam and non spam dataset in our training data.\n",
    "y=np.log(0.50)                  #x is the prob of non spam and y is the prob of of spam\n",
    "                                                   #Applying the naive bayes algorithm.We are adding the log instead of multipying due to underflow.\n",
    "z=1\n",
    "arr=[]\n",
    "for i in range(len(s)):\n",
    "    if (s[i]==z):\n",
    "        e=(k[0][t[i]])*(u[i])\n",
    "        f=(k[1][t[i]])*(u[i])\n",
    "        x+=np.log(e)\n",
    "        y+=np.log(f)\n",
    "    else:\n",
    "        z+=1\n",
    "        if x>y:\n",
    "            arr.append(0)\n",
    "        else:\n",
    "            arr.append(1)\n",
    "        x=np.log(0.50)\n",
    "        y=np.log(0.50)\n",
    "        e=(k[0][t[i]])*(u[i])\n",
    "        f=(k[1][t[i]])*(u[i])\n",
    "        x+=np.log(e)\n",
    "        y+=np.log(f)\n",
    "if x>y:\n",
    "    arr.append(0)\n",
    "else:\n",
    "    arr.append(1)\n",
    "df=pd.read_csv(\"test-labels.txt\",names = [\"LabelId\"])  #reading the file and giving them respective header.\n",
    "accuracy=0\n",
    "l=df[\"LabelId\"]\n",
    "for i in range(len(arr)):      #Comparing test label and prediction(arr)\n",
    "    if (l[i]==arr[i]):\n",
    "        accuracy+=1\n",
    "accuracy=accuracy/len(arr)\n",
    "print (\"Accuracy of the Naive Bayes Algorithm is\",accuracy*100.0)\n",
    "submission = pd.DataFrame(arr)\n",
    "submission.to_csv('prediction.txt',index = False)#Creates prediction into a new file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
